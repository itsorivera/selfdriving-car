{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Beta\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from utils import DrawLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--log-interval'], dest='log_interval', nargs=None, const=None, default=10, type=<class 'int'>, choices=None, required=False, help='interval between training status logs (default: 10)', metavar='N')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Train a PPO agent for the CarRacing-v0')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G', help='discount factor (default: 0.99)')\n",
    "parser.add_argument('--action-repeat', type=int, default=8, metavar='N', help='repeat action in N frames (default: 8)')\n",
    "parser.add_argument('--img-stack', type=int, default=4, metavar='N', help='stack N image in a state (default: 4)')\n",
    "parser.add_argument('--seed', type=int, default=0, metavar='N', help='random seed (default: 0)')\n",
    "parser.add_argument('--render', action='store_true', help='render the environment')\n",
    "parser.add_argument('--vis', action='store_true', help='use visdom')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N', help='interval between training status logs (default: 10)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mm\\Documents\\ComputerScience\\itsorivera\\ml-portfolio\\selfdriving-car\\model.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/itsorivera/ml-portfolio/selfdriving-car/model.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#args = parser.parse_args()\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/itsorivera/ml-portfolio/selfdriving-car/model.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#args, unknown = parser.parse_known_args()\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/itsorivera/ml-portfolio/selfdriving-car/model.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#args = parser.parse_known_args()[0]\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/itsorivera/ml-portfolio/selfdriving-car/model.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m args \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_args(argv[\u001b[39m1\u001b[39m:])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'argv' is not defined"
     ]
    }
   ],
   "source": [
    "#args = parser.parse_args()\n",
    "#args, unknown = parser.parse_known_args()\n",
    "#args = parser.parse_known_args()[0]\n",
    "args = parser.parse_args(argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(gamma=0.99, action_repeat=8, img_stack=4, seed=0, render=False, vis=False, log_interval=10)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--ip=127.0.0.1',\n",
       " '--stdin=9008',\n",
       " '--control=9006',\n",
       " '--hb=9005',\n",
       " '--Session.signature_scheme=\"hmac-sha256\"',\n",
       " '--Session.key=b\"69cdf743-e772-4620-ad8c-588b723bc829\"',\n",
       " '--shell=9007',\n",
       " '--transport=\"tcp\"',\n",
       " '--iopub=9009',\n",
       " '--f=c:\\\\Users\\\\mm\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-10744VTU8Di10a2p0.json']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "#device\n",
    "torch.manual_seed(args.seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = np.dtype([('s', np.float64, (args.img_stack, 96, 96)), ('a', np.float64, (3,)), ('a_logp', np.float64),\n",
    "                       ('r', np.float64), ('s_', np.float64, (args.img_stack, 96, 96))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('s', '<f8', (4, 96, 96)), ('a', '<f8', (3,)), ('a_logp', '<f8'), ('r', '<f8'), ('s_', '<f8', (4, 96, 96))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CarRacing-v2')\n",
    "        self.env.seed(args.seed)\n",
    "        \n",
    "        self.reward_threshold = self.env.spec.reward_threshold\n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        self.av_r = self.reward_memory()\n",
    "\n",
    "        self.die = False\n",
    "        img_rgb = self.env.reset()\n",
    "        img_gray = self.rgb2gray(img_rgb)\n",
    "        self.stack = [img_gray] * args.img_stack  # four frames for decision\n",
    "        return np.array(self.stack)\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        for i in range(args.action_repeat):\n",
    "            img_rgb, reward, die, _ = self.env.step(action)\n",
    "            # don't penalize \"die state\"\n",
    "            if die:\n",
    "                reward += 100\n",
    "            # green penalty\n",
    "            if np.mean(img_rgb[:, :, 1]) > 185.0:\n",
    "                reward -= 0.05\n",
    "            total_reward += reward\n",
    "            # if no reward recently, end the episode\n",
    "            done = True if self.av_r(reward) <= -0.1 else False\n",
    "            if done or die:\n",
    "                break\n",
    "        img_gray = self.rgb2gray(img_rgb)\n",
    "        self.stack.pop(0)\n",
    "        self.stack.append(img_gray)\n",
    "        assert len(self.stack) == args.img_stack\n",
    "        return np.array(self.stack), total_reward, done, die\n",
    "\n",
    "    def render(self, *arg):\n",
    "        self.env.render(*arg)\n",
    "\n",
    "    @staticmethod\n",
    "    def rgb2gray(rgb, norm=True):\n",
    "        # rgb image -> gray [0, 1]\n",
    "        gray = np.dot(rgb[..., :], [0.299, 0.587, 0.114])\n",
    "        if norm:\n",
    "            # normalize\n",
    "            gray = gray / 128. - 1.\n",
    "        return gray\n",
    "\n",
    "    @staticmethod\n",
    "    def reward_memory():\n",
    "        # record reward for last 100 steps\n",
    "        count = 0\n",
    "        length = 100\n",
    "        history = np.zeros(length)\n",
    "\n",
    "        def memory(reward):\n",
    "            nonlocal count\n",
    "            history[count] = reward\n",
    "            count = (count + 1) % length\n",
    "            return np.mean(history)\n",
    "\n",
    "        return memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic Network for PPO\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.cnn_base = nn.Sequential(  # input shape (4, 96, 96)\n",
    "            nn.Conv2d(args.img_stack, 8, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2),  # (8, 47, 47)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2),  # (16, 23, 23)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2),  # (32, 11, 11)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1),  # (64, 5, 5)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1),  # (128, 3, 3)\n",
    "            nn.ReLU(),  # activation\n",
    "        )  # output shape (256, 1, 1)\n",
    "        self.v = nn.Sequential(nn.Linear(256, 100), nn.ReLU(), nn.Linear(100, 1))\n",
    "        self.fc = nn.Sequential(nn.Linear(256, 100), nn.ReLU())\n",
    "        self.alpha_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
    "        self.beta_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
    "        self.apply(self._weights_init)\n",
    "\n",
    "    @staticmethod\n",
    "    def _weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_base(x)\n",
    "        x = x.view(-1, 256)\n",
    "        v = self.v(x)\n",
    "        x = self.fc(x)\n",
    "        alpha = self.alpha_head(x) + 1\n",
    "        beta = self.beta_head(x) + 1\n",
    "\n",
    "        return (alpha, beta), v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent():\n",
    "    \"\"\"\n",
    "    Agent for training\n",
    "    \"\"\"\n",
    "    max_grad_norm = 0.5\n",
    "    clip_param = 0.1  # epsilon in clipped loss\n",
    "    ppo_epoch = 10\n",
    "    buffer_capacity, batch_size = 2000, 128\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_step = 0\n",
    "        self.net = Net().double().to(device)\n",
    "        self.buffer = np.empty(self.buffer_capacity, dtype=transition)\n",
    "        self.counter = 0\n",
    "\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=1e-3)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).double().to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            alpha, beta = self.net(state)[0]\n",
    "        dist = Beta(alpha, beta)\n",
    "        action = dist.sample()\n",
    "        a_logp = dist.log_prob(action).sum(dim=1)\n",
    "\n",
    "        action = action.squeeze().cpu().numpy()\n",
    "        a_logp = a_logp.item()\n",
    "        return action, a_logp\n",
    "\n",
    "    def save_param(self):\n",
    "        torch.save(self.net.state_dict(), 'param/ppo_net_params.pkl')\n",
    "\n",
    "    def store(self, transition):\n",
    "        self.buffer[self.counter] = transition\n",
    "        self.counter += 1\n",
    "        if self.counter == self.buffer_capacity:\n",
    "            self.counter = 0\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def update(self):\n",
    "        self.training_step += 1\n",
    "\n",
    "        s = torch.tensor(self.buffer['s'], dtype=torch.double).to(device)\n",
    "        a = torch.tensor(self.buffer['a'], dtype=torch.double).to(device)\n",
    "        r = torch.tensor(self.buffer['r'], dtype=torch.double).to(device).view(-1, 1)\n",
    "        s_ = torch.tensor(self.buffer['s_'], dtype=torch.double).to(device)\n",
    "\n",
    "        old_a_logp = torch.tensor(self.buffer['a_logp'], dtype=torch.double).to(device).view(-1, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_v = r + args.gamma * self.net(s_)[1]\n",
    "            adv = target_v - self.net(s)[1]\n",
    "            # adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "        for _ in range(self.ppo_epoch):\n",
    "            for index in BatchSampler(SubsetRandomSampler(range(self.buffer_capacity)), self.batch_size, False):\n",
    "\n",
    "                alpha, beta = self.net(s[index])[0]\n",
    "                dist = Beta(alpha, beta)\n",
    "                a_logp = dist.log_prob(a[index]).sum(dim=1, keepdim=True)\n",
    "                ratio = torch.exp(a_logp - old_a_logp[index])\n",
    "\n",
    "                surr1 = ratio * adv[index]\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * adv[index]\n",
    "                action_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = F.smooth_l1_loss(self.net(s[index])[1], target_v[index])\n",
    "                loss = action_loss + 2. * value_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # nn.utils.clip_grad_norm_(self.net.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CarRacing' object has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mm\\Documents\\ComputerScience\\itsorivera\\ml-portfolio\\selfdriving-car\\model.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/itsorivera/ml-portfolio/selfdriving-car/model.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/itsorivera/ml-portfolio/selfdriving-car/model.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     agent \u001b[39m=\u001b[39m Agent()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/itsorivera/ml-portfolio/selfdriving-car/model.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     env \u001b[39m=\u001b[39m Env()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/itsorivera/ml-portfolio/selfdriving-car/model.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mvis:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/itsorivera/ml-portfolio/selfdriving-car/model.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         draw_reward \u001b[39m=\u001b[39m DrawLine(env\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcar\u001b[39m\u001b[39m\"\u001b[39m, title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m, xlabel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode\u001b[39m\u001b[39m\"\u001b[39m, ylabel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMoving averaged episode reward\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\mm\\Documents\\ComputerScience\\itsorivera\\ml-portfolio\\selfdriving-car\\model.ipynb Cell 13\u001b[0m in \u001b[0;36mEnv.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/itsorivera/ml-portfolio/selfdriving-car/model.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/itsorivera/ml-portfolio/selfdriving-car/model.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mCarRacing-v2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/itsorivera/ml-portfolio/selfdriving-car/model.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mseed(args\u001b[39m.\u001b[39mseed)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/itsorivera/ml-portfolio/selfdriving-car/model.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_threshold \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mspec\u001b[39m.\u001b[39mreward_threshold\n",
      "File \u001b[1;32mc:\\Users\\mm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "File \u001b[1;32mc:\\Users\\mm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "File \u001b[1;32mc:\\Users\\mm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CarRacing' object has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    env = Env()\n",
    "    if args.vis:\n",
    "        draw_reward = DrawLine(env=\"car\", title=\"PPO\", xlabel=\"Episode\", ylabel=\"Moving averaged episode reward\")\n",
    "\n",
    "    training_records = []\n",
    "    running_score = 0\n",
    "    state = env.reset()\n",
    "    for i_ep in range(100000):\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(1000):\n",
    "            action, a_logp = agent.select_action(state)\n",
    "            state_, reward, done, die = env.step(action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
    "            if args.render:\n",
    "                env.render()\n",
    "            if agent.store((state, action, a_logp, reward, state_)):\n",
    "                print('updating')\n",
    "                agent.update()\n",
    "            score += reward\n",
    "            state = state_\n",
    "            if done or die:\n",
    "                break\n",
    "        running_score = running_score * 0.99 + score * 0.01\n",
    "\n",
    "        if i_ep % args.log_interval == 0:\n",
    "            if args.vis:\n",
    "                draw_reward(xdata=i_ep, ydata=running_score)\n",
    "            print('Ep {}\\tLast score: {:.2f}\\tMoving average score: {:.2f}'.format(i_ep, score, running_score))\n",
    "            agent.save_param()\n",
    "        if running_score > env.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and the last episode runs to {}!\".format(running_score, score))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "300dbfc6b0ad0831e999f42922719e8c3270874e67c762ad6f8e521060e0a9f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
